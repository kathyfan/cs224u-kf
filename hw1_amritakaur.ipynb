{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "import utils\n",
    "import vsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM_HOME = os.path.join('data', 'vsmdata')\n",
    "DATA_HOME = os.path.join('data', 'wordrelatedness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\n",
    "    os.path.join(DATA_HOME, \"cs224u-wordrelatedness-dev.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating subword pooling methods, in this case BERT, our first big consideration was which approach to take: decontextualized or aggregated. We decided to focus on the decontextualized approach because it does not require a corpus and, as stated in lecture, produced comparable results to the aggreagated approach.\n",
    "\n",
    "Following this, we evaluated our model using various pooling functions, distance functions, and numbers of layers. In lecture and based on the papers discussed, the conclusions drawn were that fewer layers and a mean pooling function typically produced the best results. Nevertheless, we decided to test a variety of combinations of the previously stated factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decontextualized Approach Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different options for pre-trained weights. We chose to use 'bert-base-uncased' for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_weights_name = 'bert-base-uncased'\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is our implementation of some of the distance functions we tried. This includes kNearestNeighbors, as well as a function that returns the negative values of the jaccard score. We are utilizing the negative value because `vsm.create_subword_pooling_vsm` returns `-d` where `d` is the value computed by `distfunc`, since it assumes that `distfunc` is a distance value of some kind rather than a relatedness/similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "neigh = KNeighborsRegressor()\n",
    "\n",
    "def knn_distance(u, v):\n",
    "    return neigh.predict(np.concatenate((u,v), axis=1))\n",
    "\n",
    "def create_knn_model(vsm_df, dev_df, test_size=0.20):\n",
    "    X = knn_feature_matrix(vsm_df, dev_df)\n",
    "    y = dev_df['score']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    print(X_train.shape)\n",
    "    \n",
    "def knn_feature_matrix(vsm_df, rel_df):\n",
    "    matrix = np.zeros((len(rel_df), len(vsm_df.columns)*2))\n",
    "    for ind in rel_df.index:\n",
    "        matrix[ind] = knn_represent(rel_df['word1'][ind], rel_df['word2'][ind], vsm_df)\n",
    "    return matrix\n",
    "\n",
    "def knn_represent(word1, word2, vsm_df):\n",
    "    return np.concatenate((vsm_df.loc[word1], vsm_df.loc[word2]), axis=None)\n",
    "\n",
    "def neg_jaccard(u,v):\n",
    "    return -vsm.jaccard(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is our implementation of the decontextualized appraoch to BERT, in which we were able to alter the pooling function, number of layers, and distance function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bert(rel_df, layer, pool_func): \n",
    "    vocab = set(rel_df.word1.values) | set(rel_df.word2.values)\n",
    "    pooled_df = vsm.create_subword_pooling_vsm(vocab, bert_tokenizer, bert_model, layer, pool_func)\n",
    "    return pooled_df\n",
    "\n",
    "def evaluate_pooled_bert(rel_df, layer, pool_func):\n",
    "    pooled_df = apply_bert(rel_df, layer, pool_func)\n",
    "    return vsm.word_relatedness_evaluation(rel_df, pooled_df, distfunc=neg_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.43622599933657347\n",
      "2 0.3473368495338\n",
      "3 0.3279460629681185\n"
     ]
    }
   ],
   "source": [
    "pool_func = vsm.mean_pooling\n",
    "for val in range(1,4):\n",
    "    layer = val\n",
    "    pred_df, rho = evaluate_pooled_bert(dev_df, layer, pool_func)\n",
    "    print(layer, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record of pooling function, distance function, number of layers, and resulting rho\n",
    "\n",
    "| pooling func| distfunc | layer | rho |\n",
    "| --- | ---| --- | --- |\n",
    "| max | cosine | 1 | 0.2707496460162731 |\n",
    "| max | cosine | 2 | 0.20702414483988724 |\n",
    "| max | cosine | 3 | 0.17744729074571614 |\n",
    "| mean | cosine | 1 | 0.2757425333620801 |\n",
    "| mean | cosine | 2 | 0.217700456830832 |\n",
    "| mean | cosine | 3 | 0.18617500500667575 |\n",
    "| mean | euclidean | 1 | 0.28318140326817176 |\n",
    "| mean | euclidean | 2 | 0.19286314385117495 | \n",
    "| mean | euclidean | 3 | 0.1681594482646394 |\n",
    "| mean | jaccard | 1 | 0.43622599933657347 | \n",
    "| mean | jaccard | 2 | 0.3473368495338 |\n",
    "| mean | jaccard | 3 | 0.3279460629681185 |\n",
    "| min | cosine | 1 | 0.28747309266119614 |\n",
    "| min | cosine | 2 | 0.2211592952130484 | \n",
    "| min | cosine | 3 | 0.19272403506986122 |\n",
    "| min | euclidean | 1 | 0.23831264619930617 | \n",
    "| min | euclidean | 2 | 0.16104191516635505 |\n",
    "| min | euclidean | 3 | 0.1326673152179109 |\n",
    "| last | cosine | 1 | 0.26255946375943245 |\n",
    "| last | cosine | 2 | 0.20210332109799414 | \n",
    "| last | cosine | 3 | 0.1720367373470963 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
