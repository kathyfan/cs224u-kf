{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Word relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Development dataset](#Development-dataset)\n",
    "  1. [Vocabulary](#Vocabulary)\n",
    "  1. [Score distribution](#Score-distribution)\n",
    "  1. [Repeated pairs](#Repeated-pairs)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Error analysis](#Error-analysis)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [PPMI as a baseline [0.5 points]](#PPMI-as-a-baseline-[0.5-points])\n",
    "  1. [Gigaword with LSA at different dimensions [0.5 points]](#Gigaword-with-LSA-at-different-dimensions-[0.5-points])\n",
    "  1. [t-test reweighting [2 points]](#t-test-reweighting-[2-points])\n",
    "  1. [Pooled BERT representations [1 point]](#Pooled-BERT-representations-[1-point])\n",
    "  1. [Learned distance functions [2 points]](#Learned-distance-functions-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Word similarity and relatedness datasets have long been used to evaluate distributed representations. This notebook provides code for conducting such analyses with a new word relatedness datasets. It consists of word pairs, each with an associated human-annotated relatedness score. \n",
    "\n",
    "The evaluation metric for each dataset is the [Spearman correlation coefficient $\\rho$](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) between the annotated scores and your distances, as is standard in the literature. Since the train and test datasets contain both similarity and relatedness datasets, we will also report separate scores for each of these sub-tasks as well as an overall score.\n",
    "\n",
    "This homework ([questions at the bottom of this notebook](#Homework-questions)) asks you to write code that uses the count matrices in `data/vsmdata` to create and evaluate some baseline models. The final question asks you to create your own original system for this task, using any data you wish. This accounts for 9 of the 10 points for this assignment.\n",
    "\n",
    "For the associated bake-off, we will distribute a new dataset, and you will evaluate your original system (no additional training or tuning allowed!) on that datasets and submit your predictions. Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import vsm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM_HOME = os.path.join('data', 'vsmdata')\n",
    "\n",
    "DATA_HOME = os.path.join('data', 'wordrelatedness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use development dataset freely, since our bake-off evalutions involve a new test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\n",
    "    os.path.join(DATA_HOME, \"cs224u-wordrelatedness-dev.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of word pairs with scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>button</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>crane</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>ditch</td>\n",
       "      <td>0.011434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>frost</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandon</td>\n",
       "      <td>railroad</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word1     word2     score\n",
       "0  abandon    button  0.180000\n",
       "1  abandon     crane  0.160000\n",
       "2  abandon     ditch  0.011434\n",
       "3  abandon     frost  0.180000\n",
       "4  abandon  railroad  0.360000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the number of word pairs in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5012"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set will contain 1500 word pairs with scores of the same type. No word pair in the development set appears in the test set, but some of the individual words are repeated in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full vocabulary in the dataframe can be extracted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab = set(dev_df.word1.values) | set(dev_df.word2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2805"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary for the bake-off test is different â€“ it is partly overlapping with the above. If you want to be sure ahead of time that your system has a representation for every word in the dev and test sets, then you can check against the vocabularies of any of the VSMs in `data/vsmdata` (which all have the same vocabulary). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_index = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'yelp_window5-scaled.csv.gz'),\n",
    "    usecols=[0], index_col=0)\n",
    "\n",
    "full_task_vocab = list(task_index.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_task_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can process every one of those words, then you are all set. Alternatively, you can wait to see the test set and make system adjustments to ensure that you can process all those words. This is fine as long as you are not tuning your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the scores fall in $[0, 1]$, and the dataset skews towards words with low scores, meaning low relatedness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = dev_df.plot.hist().set_xlabel(\"Relatedness score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The development data has some word pairs with multiple distinct scores in it. Here we create a `pd.Series` that contains these word pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = dev_df.groupby(['word1', 'word2']).apply(lambda x: x.score.var())\n",
    "\n",
    "repeats = repeats[repeats > 0].sort_values(ascending=False)\n",
    "\n",
    "repeats.name = 'score variance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.Series` is sorted with the highest variance items at the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word1   word2 \n",
       "tiger   tiger     0.496377\n",
       "female  woman     0.483799\n",
       "cat     feline    0.448513\n",
       "buck    dollar    0.422516\n",
       "midday  noon      0.419930\n",
       "Name: score variance, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is development data, it is up to you how you want to handle these repeats. The test set has no repeated pairs in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation function is `vsm.word_relatedness_evaluation`. Its arguments:\n",
    "    \n",
    "1. A relatedness dataset `pd.DataFrame` â€“ e.g., `dev_df` as given above.\n",
    "1. A VSM `pd.DataFrame` â€“ e.g., `giga5` or some transformation thereof, or a GloVe embedding space, or something you have created on your own. The function checks that you can supply a representation for ever word in `dev_df` and raises an exception if you can't.\n",
    "1. Optionally a `distfunc` argument, which defaults to `vsm.cosine`.\n",
    "\n",
    "The function returns a tuple:\n",
    "\n",
    "1. A copy of `dev_df` with a new column giving your predictions.\n",
    "1. The Spearman $\\rho$ value (our primary score).\n",
    "\n",
    "Important note: Internally, `vsm.word_relatedness_evaluation` uses `-distfunc(x1, x2)` as its score, where `x1` and `x2` are vector representations of words. This is because the scores in our data are _positive_ relatedness scores, whereas we are assuming that `distfunc` is a _distance_ function.\n",
    "\n",
    "Here's a simple illustration using one of our count matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the original gigaword representation given\n",
    "count_df = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, \"giga_window5-scaled.csv.gz\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pred_df, count_rho = vsm.word_relatedness_evaluation(dev_df, count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08023032812449556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_rho # close to 0, i.e. very little correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>button</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>-0.336291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>crane</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>-0.307229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>ditch</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>-0.211550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>frost</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>-0.442436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandon</td>\n",
       "      <td>railroad</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>-0.363288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word1     word2     score  prediction\n",
       "0  abandon    button  0.180000   -0.336291\n",
       "1  abandon     crane  0.160000   -0.307229\n",
       "2  abandon     ditch  0.011434   -0.211550\n",
       "3  abandon     frost  0.180000   -0.442436\n",
       "4  abandon  railroad  0.360000   -0.363288"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's instructive to compare this against a truly random system, which we can create by simply having a custom distance function that returns a random number in [0, 1] for each example, making no use of the VSM itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_scorer(x1, x2):\n",
    "    \"\"\"`x1` and `x2` are vectors, to conform to the requirements\n",
    "    of `vsm.word_relatedness_evaluation`, but this function just\n",
    "    returns a random number in [0, 1].\"\"\"\n",
    "    return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008404428343748976"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pred_df, random_rho = vsm.word_relatedness_evaluation(\n",
    "    dev_df, count_df, distfunc=random_scorer)\n",
    "\n",
    "random_rho # even closer to 0 (basically random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a truly baseline system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "For error analysis, we can look at the words with the largest delta between the gold score and the distance value in our VSM. We do these comparisons based on ranks, just as with our primary metric (Spearman $\\rho$), and we normalize both rankings so that they have a comparable number of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(pred_df):\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df['relatedness_rank'] = _normalized_ranking(pred_df.prediction)\n",
    "    pred_df['score_rank'] = _normalized_ranking(pred_df.score)\n",
    "    pred_df['error'] =  abs(pred_df['relatedness_rank'] - pred_df['score_rank'])\n",
    "    return pred_df.sort_values('error')\n",
    "\n",
    "\n",
    "def _normalized_ranking(series):\n",
    "    ranks = series.rank(method='dense')\n",
    "    return ranks / ranks.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>relatedness_rank</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>boot</td>\n",
       "      <td>cliff</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>-0.204331</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>3.272272e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>governor</td>\n",
       "      <td>interview</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>-0.567041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.141903e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>conclusion</td>\n",
       "      <td>sounding</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>-0.354891</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.277154e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>glass</td>\n",
       "      <td>road</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>-0.182107</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>1.418967e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>brick</td>\n",
       "      <td>ceiling</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.132596</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>2.190320e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word1      word2     score  prediction  relatedness_rank  \\\n",
       "892         boot      cliff  0.260000   -0.204331          0.000159   \n",
       "3148    governor  interview  0.001724   -0.567041          0.000005   \n",
       "1740  conclusion   sounding  0.009619   -0.354891          0.000047   \n",
       "3121       glass       road  0.340000   -0.182107          0.000184   \n",
       "995        brick    ceiling  0.560000   -0.132596          0.000261   \n",
       "\n",
       "      score_rank         error  \n",
       "892     0.000159  3.272272e-08  \n",
       "3148    0.000005  1.141903e-07  \n",
       "1740    0.000046  1.277154e-07  \n",
       "3121    0.000184  1.418967e-07  \n",
       "995     0.000261  2.190320e-07  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis(count_pred_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>relatedness_rank</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3208</th>\n",
       "      <td>grow</td>\n",
       "      <td>sprouting</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.518391</td>\n",
       "      <td>9.015989e-06</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>grow</td>\n",
       "      <td>sprouting</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.518391</td>\n",
       "      <td>9.015989e-06</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>repeating</td>\n",
       "      <td>replicate</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-0.728052</td>\n",
       "      <td>4.507995e-07</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>repeating</td>\n",
       "      <td>replicate</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-0.728052</td>\n",
       "      <td>4.507995e-07</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>photo</td>\n",
       "      <td>photography</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.683272</td>\n",
       "      <td>1.262239e-06</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1        word2  score  prediction  relatedness_rank  score_rank  \\\n",
       "3208       grow    sprouting  0.950   -0.518391      9.015989e-06    0.000422   \n",
       "3207       grow    sprouting  0.950   -0.518391      9.015989e-06    0.000422   \n",
       "4595  repeating    replicate  0.925   -0.728052      4.507995e-07    0.000417   \n",
       "4596  repeating    replicate  0.925   -0.728052      4.507995e-07    0.000417   \n",
       "4337      photo  photography  0.940   -0.683272      1.262239e-06    0.000420   \n",
       "\n",
       "         error  \n",
       "3208  0.000413  \n",
       "3207  0.000413  \n",
       "4595  0.000417  \n",
       "4596  0.000417  \n",
       "4337  0.000418  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis(count_pred_df).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI as a baseline [0.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insight behind PPMI is a recurring theme in word representation learning, so it is a natural baseline for our task. This question asks you to write code for conducting such experiments.\n",
    "\n",
    "Your task: write a function called `run_giga_ppmi_baseline` that does the following:\n",
    "\n",
    "1. Reads the Gigaword count matrix with a window of 20 and a flat scaling function into a `pd.DataFrame`, as is done in the VSM notebooks. The file is `data/vsmdata/giga_window20-flat.csv.gz`, and the VSM notebooks provide examples of the needed code.\n",
    "1. Reweights this count matrix with PPMI.\n",
    "1. Evaluates this reweighted matrix using `vsm.word_relatedness_evaluation` on `dev_df` as defined above, with `distfunc` set to the default of `vsm.cosine`.\n",
    "1. Returns the return value of this call to `vsm.word_relatedness_evaluation`.\n",
    "\n",
    "The goal of this question is to help you get more familiar with the code in `vsm` and the function `vsm.word_relatedness_evaluation`.\n",
    "\n",
    "The function `test_run_giga_ppmi_baseline` can be used to test that you've implemented this specification correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_giga_ppmi_baseline():\n",
    "    # 1. read data\n",
    "    giga20_df = pd.read_csv(os.path.join(VSM_HOME, 'giga_window20-flat.csv.gz'), index_col=0)\n",
    "    # 2. reweight with PPMI\n",
    "    giga20_ppmi_df = vsm.pmi(giga20_df)\n",
    "    # 3-4. evaluate and return\n",
    "    return vsm.word_relatedness_evaluation(dev_df, giga20_ppmi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_giga_ppmi_baseline(func):\n",
    "    \"\"\"`func` should be `run_giga_ppmi_baseline\"\"\"\n",
    "    pred_df, rho = func()\n",
    "    rho = round(rho, 3)\n",
    "    print(rho) # sanity check the function is doing something\n",
    "    expected = 0.351\n",
    "    assert rho == expected, \\\n",
    "        \"Expected rho of {}; got {}\".format(expected, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.351\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_giga_ppmi_baseline(run_giga_ppmi_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gigaword with LSA at different dimensions [0.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might expect PPMI and LSA to form a solid pipeline that combines the strengths of PPMI with those of dimensionality reduction. However, LSA has a hyper-parameter $k$ â€“ the dimensionality of the final representations â€“ that will impact performance. This problem asks you to create code that will help you explore this approach.\n",
    "\n",
    "Your task: write a wrapper function `run_ppmi_lsa_pipeline` that does the following:\n",
    "\n",
    "1. Takes as input a count `pd.DataFrame` and an LSA parameter `k`.\n",
    "1. Reweights the count matrix with PPMI.\n",
    "1. Applies LSA with dimensionality `k`.\n",
    "1. Evaluates this reweighted matrix using `vsm.word_relatedness_evaluation` with `dev_df` as defined above. The return value of `run_ppmi_lsa_pipeline` should be the return value of this call to `vsm.word_relatedness_evaluation`.\n",
    "\n",
    "The goal of this question is to help you get a feel for how LSA can contribute to this problem. \n",
    "\n",
    "The  function `test_run_ppmi_lsa_pipeline` will test your function on the count matrix in `data/vsmdata/giga_window20-flat.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppmi_lsa_pipeline(count_df, k):\n",
    "    # 2. reweight with ppmi\n",
    "    count_ppmi_df = vsm.pmi(count_df)\n",
    "    # 3. apply lsa with dimensionality k\n",
    "    count_ppmi_lsa_df = vsm.lsa(count_ppmi_df, k=k)\n",
    "    # 4. evaluate and return \n",
    "    return vsm.word_relatedness_evaluation(dev_df, count_ppmi_lsa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_ppmi_lsa_pipeline(func):\n",
    "    \"\"\"`func` should be `run_ppmi_lsa_pipeline`\"\"\"\n",
    "    giga20 = pd.read_csv(\n",
    "        os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "    pred_df, rho = func(giga20, k=10)\n",
    "    rho = round(rho, 3)\n",
    "    print(rho)\n",
    "    expected = 0.319\n",
    "    assert rho == expected,\\\n",
    "        \"Expected rho of {}; got {}\".format(expected, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.319\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_ppmi_lsa_pipeline(run_ppmi_lsa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test reweighting [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test statistic can be thought of as a reweighting scheme. For a count matrix $X$, row index $i$, and column index $j$:\n",
    "\n",
    "$$\\textbf{ttest}(X, i, j) = \n",
    "\\frac{\n",
    "    P(X, i, j) - \\big(P(X, i, *)P(X, *, j)\\big)\n",
    "}{\n",
    "\\sqrt{(P(X, i, *)P(X, *, j))}\n",
    "}$$\n",
    "\n",
    "where $P(X, i, j)$ is $X_{ij}$ divided by the total values in $X$, $P(X, i, *)$ is the sum of the values in row $i$ of $X$ divided by the total values in $X$, and $P(X, *, j)$ is the sum of the values in column $j$ of $X$ divided by the total values in $X$.\n",
    "\n",
    "Your task: implement this reweighting scheme. You can use `test_ttest_implementation` below to check that your implementation is correct.  You do not need to use this for any evaluations, though we hope you will be curious enough to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe version\n",
    "def ttest(df):\n",
    "    # copied some code from vsm.observed_over_expected\n",
    "    col_totals = df.sum(axis=0)\n",
    "    total = col_totals.sum()\n",
    "    col_totals /= total\n",
    "    row_totals = df.sum(axis=1) / total\n",
    "    \n",
    "    P_ij = df / total\n",
    "    rowcol = np.outer(row_totals, col_totals)\n",
    "    return (P_ij - rowcol) / rowcol**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy version: the math works and it passes test_ttest_implementation\n",
    "# but it doesn't have the \"index\" of dataframe anymore.\n",
    "# keeping for my own purposes\n",
    "def ttest_numpy(df):\n",
    "    # work with numpy because it's easier\n",
    "    X = df.to_numpy()\n",
    "    \n",
    "    # get total values in X\n",
    "    df_sum_total = X.sum()\n",
    "    # get row sums, divided by total values\n",
    "    P_istar = X.sum(axis=1) / df_sum_total\n",
    "    \n",
    "    # get col sums, divided by total values\n",
    "    P_starj = X.sum(axis=0) / df_sum_total\n",
    "    \n",
    "    # construct result\n",
    "    P_ij = X / df_sum_total\n",
    "    rowcol = np.outer(P_istar, P_starj)\n",
    "    ttest = (P_ij - rowcol) / rowcol**.5\n",
    "    return ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ttest_implementation(func):\n",
    "    \"\"\"`func` should be `ttest`\"\"\"\n",
    "    X = pd.DataFrame([\n",
    "        [1.,  4.,  3.,  0.],\n",
    "        [2., 43.,  7., 12.],\n",
    "        [5.,  6., 19.,  0.],\n",
    "        [1., 11.,  1.,  4.]])\n",
    "    actual = np.array([\n",
    "        [ 0.04655, -0.01337,  0.06346, -0.09507],\n",
    "        [-0.11835,  0.13406, -0.20846,  0.10609],\n",
    "        [ 0.16621, -0.23129,  0.38123, -0.18411],\n",
    "        [-0.0231 ,  0.0563 , -0.14549,  0.10394]])\n",
    "    predicted = func(X)\n",
    "    assert np.array_equal(predicted.round(5), actual), \\\n",
    "        \"Your ttest result is\\n{}\".format(predicted.round(5))\n",
    "    print(\"ttest matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttest matched\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_ttest_implementation(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "def run_ttest_pipeline(count_df):\n",
    "    # 2. reweight with ttest\n",
    "    count_ttest_df = ttest(count_df)\n",
    "    # 4. evaluate and return \n",
    "    return vsm.word_relatedness_evaluation(dev_df, count_ttest_df)\n",
    "\n",
    "def test_run_ttest_pipeline(func):\n",
    "    \"\"\"`func` should be `run_ttest_pipeline`\"\"\"\n",
    "    giga20 = pd.read_csv(\n",
    "        os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "    pred_df, rho = func(giga20)\n",
    "    rho = round(rho, 3)\n",
    "    print(rho)\n",
    "    \n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_ttest_pipeline(run_ttest_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled BERT representations [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook [vsm_04_contextualreps.ipynb](vsm_04_contextualreps.ipynb) explores methods for deriving static vector representations of words from the contextual representations given by models like BERT and RoBERTa. The methods are due to [Bommasani et al. 2020](https://www.aclweb.org/anthology/2020.acl-main.431). The simplest of these methods involves processing the words as independent texts and pooling the sub-word representations that result, using a function like mean or max.\n",
    "\n",
    "Your task: write a function `evaluate_pooled_bert` that will enable exploration of this approach. The function should do the following:\n",
    "\n",
    "1. Take as its arguments (a) a word relatedness `pd.DataFrame` `rel_df` (e.g., `dev_df`), (b) a `layer` index (see below), and (c) a `pool_func` value (see below).\n",
    "1. Set up a BERT tokenizer and BERT model based on `'bert-base-uncased'`.\n",
    "1. Use `vsm.create_subword_pooling_vsm` to create a VSM (a `pd.DataFrame`) with the user's values for `layer` and `pool_func`.\n",
    "1. Return the return value of `vsm.word_relatedness_evaluation` using this new VSM, evaluated on `rel_df` with `distfunc` set to its default value.\n",
    "\n",
    "The function `vsm.create_subword_pooling_vsm` does the heavy-lifting. Your task is really just to put these pieces together. The result will be the start of a flexible framework for seeing how these methods do on our task. \n",
    "\n",
    "The function `test_evaluate_pooled_bert` can help you obtain the design we are seeking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def evaluate_pooled_bert(rel_df, layer, pool_func):\n",
    "    bert_weights_name = 'bert-base-uncased'\n",
    "\n",
    "    # Initialize a BERT tokenizer and BERT model based on\n",
    "    # `bert_weights_name`:\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "    bert_model = BertModel.from_pretrained(bert_weights_name)\n",
    "\n",
    "    # Get the vocabulary from `rel_df`:\n",
    "    vocab = set(rel_df.word1.values) | set(rel_df.word2.values)\n",
    "\n",
    "    # Use `vsm.create_subword_pooling_vsm` with the user's arguments:\n",
    "    pooled_df = vsm.create_subword_pooling_vsm(vocab, bert_tokenizer, bert_model, layer=layer, pool_func=pool_func)\n",
    "\n",
    "    # Return the results of the relatedness evalution:\n",
    "    return vsm.word_relatedness_evaluation(rel_df, pooled_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_pooled_bert(func):\n",
    "    import torch\n",
    "    rel_df = pd.DataFrame([\n",
    "        {'word1': 'porcupine', 'word2': 'capybara', 'score': 0.6},\n",
    "        {'word1': 'antelope', 'word2': 'springbok', 'score': 0.5},\n",
    "        {'word1': 'llama', 'word2': 'camel', 'score': 0.4},\n",
    "        {'word1': 'movie', 'word2': 'play', 'score': 0.3}])\n",
    "    layer = 2\n",
    "    pool_func = vsm.max_pooling\n",
    "    pred_df, rho = evaluate_pooled_bert(rel_df, layer, pool_func)\n",
    "    rho = round(rho, 2)\n",
    "    expected_rho = 0.40\n",
    "    assert rho == expected_rho, \\\n",
    "        \"Expected rho={}; got rho={}\".format(expected_rho, rho)\n",
    "    print(\"bert matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert matched\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_evaluate_pooled_bert(evaluate_pooled_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned distance functions [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presentation thus far leads one to assume that the `distfunc` argument used in the experiments will be a standard vector distance function like `vsm.cosine` or `vsm.euclidean`. However, the framework itself simply requires that this function map two fixed-dimensional vectors to a real number. This opens up a world of possibilities. This question asks you to dip a toe in these waters.\n",
    "\n",
    "Your task: write a function `run_knn_score_model` for models in this class. The function should:\n",
    "\n",
    "1. Take as its arguments (a) a VSM dataframe `vsm_df`, (b) a relatedness dataset (e.g., `dev_df`), and (c) a `test_size` value between 0.0 and 1.0 that can be passed directly to `train_test_split` (see below).\n",
    "1. Create a feature matrix `X`: each word pair in `dev_df` should be represented by the concatenation of the vectors for word1 and word2 from `vsm_df`.\n",
    "1. Create a score vector `y`, which is just the `score` column in `dev_df`.\n",
    "1. Split the dataset `(X, y)` into train and test portions using [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "1. Train an [sklearn.neighbors.KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) model on the train split from step 4, with default hyperparameters.\n",
    "1. Return the value of the `score` method of the trained `KNeighborsRegressor` model on the test split from step 4.\n",
    "\n",
    "The functions `test_knn_feature_matrix` and `knn_represent` will help you test the crucial representational aspects of this.\n",
    "\n",
    "Note: if you decide to apply this approach to our task as part of an original system, recall that `vsm.create_subword_pooling_vsm` returns `-d` where `d` is the value computed by `distfunc`, since it assumes that `distfunc` is a distance value of some kind rather than a relatedness/similarity value. Since most regression models will return positive scores for positive associations, you will probably want to undue this by having your `distfunc` return the negative of its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['w2' 'w3']\n",
      "[1. 2. 3. 4.]\n",
      "[4. 5. 6. 7.]\n",
      "[1. 2. 3. 4. 4. 5. 6. 7.]\n",
      "[1. 2. 3. 4.]\n",
      "[ 7.  8.  9. 10.]\n",
      "[ 1.  2.  3.  4.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "# SCRATCH: some manual testing to get components of knn working\n",
    "rel_df = pd.DataFrame([\n",
    "        {'word1': 'w1', 'word2': 'w2', 'score': 0.1},\n",
    "        {'word1': 'w1', 'word2': 'w3', 'score': 0.2}])\n",
    "vsm_df = pd.DataFrame([\n",
    "        [1, 2, 3., 4.],\n",
    "        [4, 5, 6., 7.],\n",
    "        [7, 8, 9., 10.]], index=['w1', 'w2', 'w3'])\n",
    "print(vsm_df.to_numpy().shape[1])\n",
    "\n",
    "word1s = rel_df.word1.values\n",
    "word2s = rel_df.word2.values\n",
    "print(word2s)\n",
    "for i in range(len(word2s)):\n",
    "    print(vsm_df.loc[word1s[i]].to_numpy())\n",
    "    print(vsm_df.loc[word2s[i]].to_numpy())\n",
    "    print(np.concatenate((vsm_df.loc[word1s[i]].to_numpy(),vsm_df.loc[word2s[i]].to_numpy())))\n",
    "# print(word1s, word1s.shape)\n",
    "# y = rel_df.score\n",
    "# print(y.shape)\n",
    "\n",
    "# print(rel_df.shape[0])\n",
    "# print(len(word1s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def run_knn_score_model(vsm_df, dev_df, test_size=0.20):\n",
    "    # Complete `knn_feature_matrix` for this step.\n",
    "    X = knn_feature_matrix(vsm_df, dev_df)\n",
    " \n",
    "    # Get the values of the 'score' column in `dev_df`\n",
    "    # and store them in a list or array `y`.\n",
    "    y = dev_df.score\n",
    "\n",
    "    # Use `train_test_split` to split (X, y) into train and\n",
    "    # test portions, with `test_size` as the test size.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size)\n",
    "\n",
    "    # Instantiate a `KNeighborsRegressor` with default arguments:\n",
    "    neigh = KNeighborsRegressor()\n",
    "\n",
    "    # Fit the model on the training data:\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # Return the value of `score` for your model on the test split\n",
    "    # you created above:\n",
    "    return neigh.score(X_test, y_test)\n",
    "\n",
    "# Complete `knn_represent` and use it to create a feature\n",
    "# matrix `np.array`:\n",
    "def knn_feature_matrix(vsm_df, rel_df):\n",
    "    num_pairs = rel_df.shape[0]\n",
    "    word1s = rel_df.word1.values\n",
    "    word2s = rel_df.word2.values\n",
    "    vec_length = vsm_df.to_numpy().shape[1] * 2\n",
    "    X = np.empty((num_pairs,vec_length))\n",
    "    \n",
    "    for i in range(num_pairs):\n",
    "        word1 = word1s[i]\n",
    "        word2 = word2s[i]\n",
    "        X[i] = knn_represent(word1, word2, vsm_df)\n",
    "    return X\n",
    "\n",
    "# Use `vsm_df` to get vectors for `word1` and `word2`\n",
    "# and concatenate them into a single vector:\n",
    "def knn_represent(word1, word2, vsm_df):\n",
    "    word1vec = vsm_df.loc[word1].to_numpy()\n",
    "    word2vec = vsm_df.loc[word2].to_numpy()\n",
    "    return np.concatenate((word1vec,word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn_feature_matrix(func):\n",
    "    rel_df = pd.DataFrame([\n",
    "        {'word1': 'w1', 'word2': 'w2', 'score': 0.1},\n",
    "        {'word1': 'w1', 'word2': 'w3', 'score': 0.2}])\n",
    "    vsm_df = pd.DataFrame([\n",
    "        [1, 2, 3.],\n",
    "        [4, 5, 6.],\n",
    "        [7, 8, 9.]], index=['w1', 'w2', 'w3'])\n",
    "    expected = np.array([\n",
    "        [1, 2, 3, 4, 5, 6.],\n",
    "        [1, 2, 3, 7, 8, 9.]])\n",
    "    result = func(vsm_df, rel_df)\n",
    "    assert np.array_equal(result, expected), \\\n",
    "        \"Your `knn_feature_matrix` returns: {}\\nWe expect: {}\".format(\n",
    "        result, expected)\n",
    "    print(\"knn matrix passed!\")\n",
    "\n",
    "def test_knn_represent(func):\n",
    "    vsm_df = pd.DataFrame([\n",
    "        [1, 2, 3.],\n",
    "        [4, 5, 6.],\n",
    "        [7, 8, 9.]], index=['w1', 'w2', 'w3'])\n",
    "    result = func('w1', 'w3', vsm_df)\n",
    "    expected = np.array([1, 2, 3, 7, 8, 9.])\n",
    "    assert np.array_equal(result, expected), \\\n",
    "        \"Your `knn_represent` returns: {}\\nWe expect: {}\".format(\n",
    "        result, expected)\n",
    "    print(\"knn represent passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn represent passed!\n",
      "knn matrix passed!\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_knn_represent(knn_represent)\n",
    "    test_knn_feature_matrix(knn_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure how to actually use this dist func...\n",
    "# def constant_dist(u, v):\n",
    "#     giga20 = pd.read_csv(\n",
    "#         os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "#     return np.sum(np.minimum(u, v))*0 + run_knn_score_model(giga20, dev_df, test_size=0.20) * -1.0\n",
    "\n",
    "# def run_knn_pipeline(count_df):\n",
    "#     #dist = run_knn_score_model(count_df, dev_df, test_size=0.20) * -1.0\n",
    "#     #dist = 1 # constant func\n",
    "#     dist = constant_dist\n",
    "#     return vsm.word_relatedness_evaluation(dev_df, count_df, distfunc=dist)\n",
    "\n",
    "# def test_run_knn_pipeline(func):\n",
    "#     \"\"\"`func` should be `run_knn_pipeline`\"\"\"\n",
    "#     giga20 = pd.read_csv(\n",
    "#         os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "#     pred_df, rho = func(giga20)\n",
    "#     rho = round(rho, 3)\n",
    "#     print(rho)\n",
    "    \n",
    "# if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "#     test_run_knn_pipeline(run_knn_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own model. You can of course include steps made above (ideally, the above questions informed your system design!), but your model should not be literally identical to any of the above models. Other ideas: retrofitting, autoencoders, GloVe, subword modeling, ... \n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your system must work with `vsm.word_relatedness_evaluation`. You are free to specify the VSM and the value of `distfunc`.\n",
    "\n",
    "1. Your code must be self-contained, so that we can work with your model directly in your homework submission notebook. If your model depends on external data or other resources, please submit a ZIP archive containing these resources along with your submission.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. We also ask that you report the best score your system got during development, just to help us understand how systems performed overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converged at iteration 4; change was 0.0026 "
     ]
    }
   ],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "#   3) The score achieved by your system in place of MY_NUMBER.\n",
    "#        With no other changes to that line.\n",
    "#        You should report your score as a decimal value <=1.0\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# NOTE: MODULES, CODE AND DATASETS REQUIRED FOR YOUR ORIGINAL SYSTEM\n",
    "# SHOULD BE ADDED BELOW THE 'IS_GRADESCOPE_ENV' CHECK CONDITION. DOING\n",
    "# SO ABOVE THE CHECK MAY CAUSE THE AUTOGRADER TO FAIL.\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "# My peak score was: 0.3812425385752641\n",
    "# Textual description: our open model is a PPMI - LSA - Retrofitting pipeline.\n",
    "# The output of the model is retro_df, a custom vsm.\n",
    "# For extensive description + record of our other experiments, see files:\n",
    "# https://github.com/kathyfan/cs224u-kf/blob/main/hw1_kathyzfan.ipynb\n",
    "# https://github.com/kathyfan/cs224u-kf/blob/main/hw1_amritakaur.ipynb\n",
    "# https://github.com/kathyfan/cs224u-kf/blob/main/hw1_maoguo.ipynb\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    import retrofitting\n",
    "    from retrofitting import Retrofitter\n",
    "\n",
    "    dev_df = pd.read_csv(os.path.join(DATA_HOME, \"cs224u-wordrelatedness-dev.csv\"))\n",
    "    count_df = pd.read_csv(os.path.join(VSM_HOME, 'giga_window20-flat.csv.gz'), index_col=0)\n",
    "    \n",
    "    def apply_ppmi(df):\n",
    "        return vsm.pmi(df)\n",
    "    \n",
    "    def apply_lsa(df, k):\n",
    "        return vsm.lsa(df, k=k)\n",
    "\n",
    "    def ppmi_lsa_model(k):\n",
    "        # print(k)\n",
    "        ppmi = apply_ppmi(count_df)\n",
    "        pipeline = apply_lsa(ppmi, k)\n",
    "        return pipeline\n",
    "    \n",
    "    def get_wordnet_edges():\n",
    "        edges = defaultdict(set)\n",
    "        for ss in wn.all_synsets():\n",
    "            lem_names = {lem.name() for lem in ss.lemmas()}\n",
    "            for lem in lem_names:\n",
    "                edges[lem] |= lem_names\n",
    "        return edges\n",
    "\n",
    "    def convert_edges_to_indices(edges, Q):\n",
    "        lookup = dict(zip(Q.index, range(Q.shape[0])))\n",
    "        index_edges = defaultdict(set)\n",
    "        for start, finish_nodes in edges.items():\n",
    "            s = lookup.get(start)\n",
    "            if s:\n",
    "                f = {lookup[n] for n in finish_nodes if n in lookup}\n",
    "                if f:\n",
    "                    index_edges[s] = f\n",
    "        return index_edges\n",
    "    \n",
    "    def evaluate(df):\n",
    "        pred_df, rho = vsm.word_relatedness_evaluation(dev_df, df)\n",
    "        # print(rho)\n",
    "        return rho\n",
    "    \n",
    "    ppmi_lsa = ppmi_lsa_model(k=100)\n",
    "    wn_edges = get_wordnet_edges()\n",
    "    wn_index_edges = convert_edges_to_indices(wn_edges, ppmi_lsa)\n",
    "    wn_retro = Retrofitter(alpha=lambda x: 10, verbose=True)\n",
    "    retro_df = wn_retro.fit(ppmi_lsa, wn_index_edges)\n",
    "    # evaluate(retro_df)\n",
    "    # retro_df.to_csv(\"retro_df.csv\")\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, you simply need to evaluate your original system on the file \n",
    "\n",
    "`vsmdata/cs224u-wordrelatedness-bakeoff-test-unlabeled.csv`\n",
    "\n",
    "This contains only word pairs (no scores), so `vsm.word_relatedness_evaluation` will simply make predictions without doing any scoring. Use that function to make predictions with your original system, store the resulting `pred_df` to a file, and then upload the file as your bake-off submission.\n",
    "\n",
    "The following function should be used to conduct this evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bakeoff_submission(\n",
    "        vsm_df,\n",
    "        distfunc,\n",
    "        output_filename=\"cs224u-wordrelatedness-bakeoff-entry.csv\"):\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(DATA_HOME, \"cs224u-wordrelatedness-test-unlabeled.csv\"))\n",
    "\n",
    "    pred_df, _ = vsm.word_relatedness_evaluation(test_df, vsm_df, distfunc=distfunc)\n",
    "\n",
    "    pred_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if `count_df` were the VSM for my system, and I wanted my distance function to be `vsm.euclidean`, I would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# retro_df_from_csv = test_df = pd.read_csv(\n",
    "#         \"retro_df.csv\")\n",
    "# create_bakeoff_submission(retro_df, vsm.cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a file `cs224u-wordrelatedness-bakeoff-entry.csv` in the current directory. That file should be uploaded as-is. Please do not change its name.\n",
    "\n",
    "Only one upload per team is permitted, and you should do no tuning of your system based on what you see in `pred_df` â€“ you should not study that file in anyway, beyond perhaps checking that it contains what you expected it to contain. The upload function will do some additional checking to ensure that your file is well-formed.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
