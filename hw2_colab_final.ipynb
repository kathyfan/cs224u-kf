{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw2_colab_final.ipynb","provenance":[],"authorship_tag":"ABX9TyNcR6JFOtkIOgN78YKwcrTQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9nVviN5G0sRw"},"source":["# Summary of model\n","TL;DR: Our group noticed a critical weakness in predicting SST-3 neutral labels fro most models from the course notebooks, so we took the approach of trying to boost the prediction for this particular label.\n","\n","Since this seemed to be a gap experienced by all the models, we were extremely interested in investigating how we could engineer a particular training set/training regime that could help (any) model overcome this. Thus, we started with a model that seems to be doing well, and trying to optimize the training data in order to make the model better at its weakest prediction.\n","\n","First, we started by surveying the examples that were already included in the course notebooks in a [spreadsheet](https://docs.google.com/spreadsheets/d/18TpQ84CP4cQvLGLX-abzg06h3195AoURdKaGzb8wuU8/edit?usp=sharing&resourcekey=0-XMaX_Xj3pEve0AMA4-aH2g). We saw three key observations:\n","- training on DynaSent dev was very helpful, increasing the score of a particular model by 0.1 compared to training on SST-3 alone.\n","- BERT vectors offered a >0.1 point improvement over glove vectors; generally the highest-scoring models so far were ones that used BERT encoding.\n","- the model that performs the best so far uses BERT encoding and finetuning.\n","\n","Taking these results, we set forth to optimize a model based on BERT encoding and a finetuned classifier."]},{"cell_type":"markdown","metadata":{"id":"hBMJuGNO21m1"},"source":["# Optimizing the training data\n","\n","For all our experiments, we used SST-3-dev and DynaSent-dev as the assessment dataframes. We observed that our base BERT + RNN classifier struggled the most with predicting the neutral label on SST-3. Since the macro-F1 score weighs each class and each dataset equally, this affected the final performance a lot. We were hoping that if we could focus on tuning the predictions in this category, we'd be able to get a score in the 0.72 range, since the model can achieve this average score for the other labels/dataset. \n","\n","## Step 1: add DynaSent round 1 data\n","We noted that SST-3-train had label distribution of (neg, neutral, pos)=(3310, 1624, 3610). We imported the DynaSent round 1 data and used a subset of it, with label distribution of (neg, neutral, pos)=(2000, 4000, 2000). This allowed us to balance out the label distribution in the entire training set. Further, it meant we had roughly the same amount of SST-3 and DynaSent data to train with. (This is why we did not just use the entire round 1 dataset.) We chose to use the round 1 data bacause our BERT+RNN model is likely less robust than the RoBERTa model used in the DynaSent paper, and we reasoned that the round 1 data was sufficiently \"difficult\" for our model to learn and improve from.\n","\n","We did not see as noticeable of an improvement in the neutral category from this addition, so...\n","\n","## Step 2: add SST-3 subtree data\n","We decided to add more SST-3 data to the training set as well. Here, we loaded the subtree version of SST-3-train and took the first 3000 examples that had `label=neutral`. The idea was to boost the number of neutral SST-3 examples, without overwhelming the dataset with neutral labels, to avoid confusing the model with these more-difficult examples.\n","\n","We noticed a 0.1 improvement in the neutral label f1-score after this addition. Furthermore, whereas the model we started with had roughly a 0.6 difference in the macro average f-1 scores of SST-3 vs DynaSent, using this training data, our model had very similar macro average f-1 scores for these two datasets. This was a positive change, potentially indicating a model with better domain-transfer abilities.\n","\n","We tried increasing the number of subtree neutral examples to see if it would lend an even greater improvement, but the model started to compromise in its score for the negative label, so we decided to stick with 3000.\n","\n","Our final training data consisted of SST-3 train root-level data, 3000 of SST-3 train subtree examples, and 8000 of DynaSent round 1 examples.\n","\n","## Step 3: explore different training regimes\n","We realized that by simply concatenating the above three data sources into our training set, the training examples were presented in some sort of an order.\n","\n","In other papers, we've read about how different training regimes, for example showing the model easier examples first and harder ones later, or pretraining the model with a slightly different task, could affect its learning and performance. Therefore, some variations we tried were:\n","* Scrambling the entire dataframe. \n","  * Hypothesis: this could help bridge the gap between SST-3 neutral and DynaSent neutral label predictions, since all the data was mixed up and the model would not be \"biased\" towards learning one of those representations\n","  * Result: this decreased the performance. \n","* Train in different orders, such as [DynaSent, SST-3 root, SST-3 subtree neutrals] and [DynaSent, SST-3 subtree neutrals, SST-3 root]. Most of these permutations also decreased the score. Our final model is trained with data in the order of [SST-3 root, SST-3 subtree, DynaSent]. We found that this permutation was the one that gave the best results. \n","\n","## Side note\n","Another reason for using subsets of the DynaSent and subtree data was that our group could not make use of Colab's GPU properly and each experiment took 40-60 minutes to run. Therefore, we wanted to keep the data size manageable while being able to observe some results."]},{"cell_type":"markdown","metadata":{"id":"S9N1T313RpC7"},"source":["# Hyperparameter tuning\n","\n","From lecture, we noted that oftentimes the choice of feature function makes the most difference in the performance of the model. Since we wanted to use BERT features, we were not able to tweak this in that many other ways, which is another reason why we chose to instead focus more on studying the effects of the training data. \n","\n","However, we did perform some hyperparameter tuning on the model. The parameters we tuned were hidden dim, eta, batch size, gradient accumulation steps, set of BERT weights, and activation function. We found that compared to the class defaults, a larger hidden dimension and smaller eta were better. The batch size was reduced due to memory constraint, and correspondingly, the model performed better with a larger gradient accumulation step, since this smoothes out variability from the small batches. We found that the cased BERT weights worked better than uncased weights, and the Tanh activation function performed better than others tried. "]},{"cell_type":"markdown","metadata":{"id":"1YucT5tW7k7Y"},"source":["## Below is the code for our model."]},{"cell_type":"markdown","metadata":{"id":"x09-_fEcRb4u"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"I2K4ow0wAAEh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618954184980,"user_tz":420,"elapsed":25508,"user":{"displayName":"Kathy Fan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDu6QQ-4Y3vtdNwnEo4nRL8leels8l0AF-0uYv=s64","userId":"15836587714405057231"}},"outputId":"7b1d762b-169c-42e5-eef7-9ccedabd0f52"},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder\n","FOLDERNAME = 'personal/CS224U/cs224u-kf/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# this ensures that the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jikHsrpgVp4Q","executionInfo":{"status":"ok","timestamp":1618954191956,"user_tz":420,"elapsed":19432,"user":{"displayName":"Kathy Fan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDu6QQ-4Y3vtdNwnEo4nRL8leels8l0AF-0uYv=s64","userId":"15836587714405057231"}},"outputId":"05662577-0a0f-4cb5-8235-1c761fa10f63"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 6.0MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 23.2MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 38.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i9ED4fOEULIF"},"source":["import os\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","\n","from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n","from torch_rnn_classifier import TorchRNNModel\n","from torch_rnn_classifier import TorchRNNClassifier\n","from torch_rnn_classifier import TorchRNNClassifierModel\n","from torch_rnn_classifier import TorchRNNClassifier\n","import sst\n","import utils\n","\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFHGfdhARfhe"},"source":["## Training data setup"]},{"cell_type":"code","metadata":{"id":"IYQICkMFUSyW"},"source":["utils.fix_random_seeds()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FK90Y3VUT32"},"source":["SST_HOME = os.path.join(sys.path[-1], 'data/sentiment')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQI9LLG01m99"},"source":["bakeoff_dev = sst.bakeoff_dev_reader(SST_HOME)\n","sst_dev = sst.dev_reader(SST_HOME)\n","sst_train = sst.train_reader(SST_HOME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_U9gTiHZJS1"},"source":["# load DynaSent dataset\n","# want format:\n","# (example_id, sentence, label, is_subtree)\n","# use text_id for example_id\n","# sentence = sentence\n","# gold_label -> label\n","# is_subtree = 0\n","import json\n","\n","def load_dataset(*src_filenames, labels=None):\n","    data = []\n","    for filename in src_filenames:\n","        with open(filename) as f:\n","            for line in f:\n","                d = json.loads(line)\n","                if labels is None or d['gold_label'] in labels:\n","                    data.append(d)\n","    return data\n","\n","dynasent_folder = os.path.join(sys.path[-1], 'data/dynasent-v1.1')\n","r1_train_filename = os.path.join(dynasent_folder, 'dynasent-v1.1-round01-yelp-train.jsonl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wr-Ztmbaq2tY","executionInfo":{"status":"ok","timestamp":1619021165671,"user_tz":420,"elapsed":18136,"user":{"displayName":"Kathy Fan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDu6QQ-4Y3vtdNwnEo4nRL8leels8l0AF-0uYv=s64","userId":"15836587714405057231"}},"outputId":"8f07d32c-558c-444d-b8ed-070f95fd5577"},"source":["# get a subsample of each label\n","r1_train_neg = load_dataset(r1_train_filename, labels=('negative', 'negative'))[:2000]\n","r1_train_neu = load_dataset(r1_train_filename, labels=('neutral', 'neutral'))[:2000]\n","r1_train_pos = load_dataset(r1_train_filename, labels=('positive', 'positive'))[:4000]\n","\n","pairs_neg = zip((d['text_id'], d['sentence'], d['gold_label'], 0) for d in r1_train_neg) \n","pairs_list_neg = list(pairs_neg)\n","df_source_neg = [pair[0] for pair in pairs_list_neg]\n","df_neg = pd.DataFrame(df_source_neg, columns =['example_id', 'sentence', 'label', 'is_subtree'])\n","\n","pairs_neu = zip((d['text_id'], d['sentence'], d['gold_label'], 0) for d in r1_train_neu) \n","pairs_list_neu = list(pairs_neu)\n","df_source_neu = [pair[0] for pair in pairs_list_neu]\n","df_neu = pd.DataFrame(df_source_neu, columns =['example_id', 'sentence', 'label', 'is_subtree'])\n","\n","pairs_pos = zip((d['text_id'], d['sentence'], d['gold_label'], 0) for d in r1_train_pos) \n","pairs_list_pos = list(pairs_pos)\n","df_source_pos = [pair[0] for pair in pairs_list_pos]\n","df_pos = pd.DataFrame(df_source_pos, columns =['example_id', 'sentence', 'label', 'is_subtree'])\n","\n","# concatenate all labels\n","df_whole = pd.concat([df_neg, df_neu, df_pos])\n","print(len(df_whole))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lRSQ6jAzLav9"},"source":["# try using subtrees SST neutral examples --\n","# SST-train contains 8544 examples; (neg, neutral, pos)=(3310, 1624, 3610)\n","subtree_dedup_train_df = sst.train_reader(SST_HOME, include_subtrees=True, dedup=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_6oejl030Pv"},"source":["# use 3000 additional neutral examples.\n","subtree_neutrals = subtree_dedup_train_df.loc[subtree_dedup_train_df['label'] == 'neutral'][:3000]\n","sst_boosted_ds_train = pd.concat([sst_train, subtree_neutrals, df_whole])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Y5ig5LTRirG"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"NQ2bJVl8_5Sl"},"source":["class HfBertClassifierModel(nn.Module):\n","    def __init__(self, n_classes, weights_name='bert-base-cased'):\n","        super().__init__()\n","        self.n_classes = n_classes\n","        self.weights_name = weights_name\n","        self.bert = BertModel.from_pretrained(self.weights_name)\n","        self.bert.train()\n","        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n","        # The only new parameters -- the classifier:\n","        self.classifier_layer = nn.Linear(\n","            self.hidden_dim, self.n_classes)\n","\n","    def forward(self, indices, mask):\n","        reps = self.bert(\n","            indices, attention_mask=mask)\n","        return self.classifier_layer(reps.pooler_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqboxc7v3He0"},"source":["class HfBertClassifier(TorchShallowNeuralClassifier):\n","    def __init__(self, weights_name, *args, **kwargs):\n","        self.weights_name = weights_name\n","        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n","        super().__init__(*args, **kwargs)\n","        self.params += ['weights_name']\n","\n","    def build_graph(self):\n","        return HfBertClassifierModel(self.n_classes_, self.weights_name)\n","\n","    def build_dataset(self, X, y=None):\n","        data = self.tokenizer.batch_encode_plus(\n","            X,\n","            max_length=None,\n","            add_special_tokens=True,\n","            padding='longest',\n","            return_attention_mask=True)\n","        indices = torch.tensor(data['input_ids'])\n","        mask = torch.tensor(data['attention_mask'])\n","        if y is None:\n","            dataset = torch.utils.data.TensorDataset(indices, mask)\n","        else:\n","            self.classes_ = sorted(set(y))\n","            self.n_classes_ = len(self.classes_)\n","            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n","            y = [class2index[label] for label in y]\n","            y = torch.tensor(y)\n","            dataset = torch.utils.data.TensorDataset(indices, mask, y)\n","        return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJG9rOqgUJ3F"},"source":["def bert_fine_tune_phi(text):\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4W9OGVDQsWgp"},"source":["# This was first written as a version containing hyperparameter search\n","# and modified to use the final best version from the search\n","def fit_hf_bert_classifier(X, y):\n","    mod = HfBertClassifier(\n","        gradient_accumulation_steps=8,\n","        eta=0.0001,\n","        hidden_dim=300,\n","        weights_name='bert-base-cased', # also try bert-based-uncased\n","        batch_size=8,  # Small batches to avoid memory overload.\n","        max_iter=1,  # We'll search based on 1 iteration for efficiency.\n","        n_iter_no_change=5,   # Early-stopping params are for the\n","        early_stopping=True)  # final evaluation.\n","\n","    mod.fit(X, y)\n","\n","    return mod"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtwTonha3RKF"},"source":["sst_boosted_ds_train = pd.concat([sst_train, subtree_neutrals, df_whole]) # examples are NOT shuffled\n","\n","# took around an hour maybe to run\n","# without gpu: still has not completed after 5h\n","bert_classifier_xval = sst.experiment(\n","    sst_boosted_ds_train,\n","    bert_fine_tune_phi,\n","    fit_hf_bert_classifier,\n","    assess_dataframes=[sst_dev, bakeoff_dev],\n","    vectorize=False)  # Pass in the BERT hidden state directly!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1fhIL133Ui0"},"source":["optimized_bert_classifier = bert_classifier_xval['model']\n","\n","# Remove the rest of the experiment results to clear out some memory\n","del bert_classifier_xval"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQR-caYd3js_"},"source":["def fit_optimized_hf_bert_classifier(X, y):\n","    optimized_bert_classifier.max_iter = 1000\n","    optimized_bert_classifier.fit(X, y)\n","    return optimized_bert_classifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rARQwx8Y3mcK","executionInfo":{"status":"ok","timestamp":1618985435010,"user_tz":420,"elapsed":12916193,"user":{"displayName":"Kathy Fan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDu6QQ-4Y3vtdNwnEo4nRL8leels8l0AF-0uYv=s64","userId":"15836587714405057231"}},"outputId":"a919dd57-8839-40ff-a395-382ff0a11377"},"source":["# took 3.5h to run....\n","hfbert_experiment = sst.experiment(\n","    sst_boosted_ds_train, \n","    bert_fine_tune_phi,\n","    fit_optimized_hf_bert_classifier,\n","    assess_dataframes=[sst_dev, bakeoff_dev],\n","    vectorize=False)  # Pass in the BERT hidden state directly!"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Stopping after epoch 10. Validation score did not improve by tol=1e-05 for more than 5 epochs. Final error is 17.4847345644921"],"name":"stderr"},{"output_type":"stream","text":["Assessment dataset 1\n","              precision    recall  f1-score   support\n","\n","    negative      0.775     0.734     0.754       428\n","     neutral      0.427     0.345     0.382       229\n","    positive      0.732     0.842     0.783       444\n","\n","    accuracy                          0.697      1101\n","   macro avg      0.645     0.640     0.640      1101\n","weighted avg      0.685     0.697     0.688      1101\n","\n","Assessment dataset 2\n","              precision    recall  f1-score   support\n","\n","    negative      0.606     0.703     0.651       565\n","     neutral      0.818     0.511     0.629      1019\n","    positive      0.594     0.817     0.688       777\n","\n","    accuracy                          0.658      2361\n","   macro avg      0.673     0.677     0.656      2361\n","weighted avg      0.694     0.658     0.654      2361\n","\n","Mean of macro-F1 scores: 0.648\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"thQEygbj-euT"},"source":["def predict_one_rnn(text):\n","    # List of tokenized examples:\n","    X = [hfbert_experiment['phi'](text)]\n","    # Standard `predict` step on a list of lists of str:\n","    preds = hfbert_experiment['model'].predict(X)\n","    # Be sure to return the only member of the predictions,\n","    # rather than the singleton list:\n","    return preds[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hg_i-pvU-xfQ"},"source":["def create_bakeoff_submission(\n","        predict_one_func,\n","        output_filename='cs224u-sentiment-bakeoff-entry.csv'):\n","\n","    bakeoff_test = sst.bakeoff_test_reader(SST_HOME)\n","    sst_test = sst.test_reader(SST_HOME)\n","    bakeoff_test['dataset'] = 'bakeoff'\n","    sst_test['dataset'] = 'sst3'\n","    df = pd.concat((bakeoff_test, sst_test))\n","\n","    df['prediction'] = df['sentence'].apply(predict_one_func)\n","\n","    df.to_csv(output_filename, index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yhkww8y-2wz"},"source":["create_bakeoff_submission(predict_one_rnn)"],"execution_count":null,"outputs":[]}]}